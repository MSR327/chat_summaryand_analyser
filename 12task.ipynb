{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO6iScHzPo7w",
        "outputId": "dd278a5a-bac6-4ec5-f08b-5cd67ea0d603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import os\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "f0Jdtr4OPwoT"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = getpass(\"from getpass import getpass\")\n",
        "os.environ[\"GROQ_API_KEY\"] = api_key\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk_3W-GEP2XJ",
        "outputId": "45136a0c-038c-49b4-b50f-df8df6c1694c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from getpass import getpass··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ConversationManager:\n",
        "    def __init__(self, summarize_every=3):\n",
        "        self.history = []\n",
        "        self.run_count = 0\n",
        "        self.summarize_every = summarize_every\n",
        "\n",
        "    def add_user_message(self, message):\n",
        "        self.history.append({\"role\": \"user\", \"content\": message})\n",
        "        self.run_count += 1\n",
        "        self.truncate_by_turns(5)\n",
        "        if self.run_count % self.summarize_every == 0:\n",
        "            self.summarize_history()\n",
        "        reply = self.generate_reply()\n",
        "        self.history.append({\"role\": \"assistant\", \"content\": reply})\n",
        "        return reply\n",
        "\n",
        "    def generate_reply(self):\n",
        "        context = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in self.history])\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            messages=[{\"role\": \"user\", \"content\": context}]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def truncate_by_turns(self, n=5):\n",
        "        self.history = self.history[-n:]\n",
        "\n",
        "    def truncate_by_length(self, max_chars=300):\n",
        "        text = \"\"\n",
        "        truncated = []\n",
        "        for msg in reversed(self.history):\n",
        "            if len(text) + len(msg[\"content\"]) > max_chars:\n",
        "                break\n",
        "            truncated.insert(0, msg)\n",
        "            text += msg[\"content\"]\n",
        "        self.history = truncated\n",
        "\n",
        "    def summarize_history(self):\n",
        "        prompt = \"Summarize this conversation in 2-3 sentences:\\n\"\n",
        "        for msg in self.history:\n",
        "            prompt += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        summary = response.choices[0].message.content\n",
        "        self.history = [{\"role\": \"system\", \"content\": f\"Summary: {summary}\"}]\n",
        "\n",
        "    def get_history(self):\n",
        "        return self.history"
      ],
      "metadata": {
        "id": "0aedNkC2Tw74"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2ylz8q_aqIEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatAnalyzerOptimized:\n",
        "    def __init__(self, client, model=\"llama-3.1-8b-instant\"):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "\n",
        "    def analyze_messages(self, messages):\n",
        "        system_prompt = \"\"\"\n",
        "        You are an assistant that classifies and extracts structured data from user messages.\n",
        "        For each message, provide:\n",
        "          - category: greeting, question, feedback, complaint, preference, other\n",
        "          - extracted_data: Python dict with fields 'genre' and 'character_type'\n",
        "        Respond only with a Python list of dictionaries, one per message:\n",
        "        Example:\n",
        "        [\n",
        "          {\"message\": \"Hi there!\", \"category\": \"greeting\", \"extracted_data\": {\"genre\": \"\", \"character_type\": \"\"}},\n",
        "          {\"message\": \"I like action movies with strong lead characters.\", \"category\": \"preference\", \"extracted_data\": {\"genre\": \"action\", \"character_type\": \"strong lead\"}}\n",
        "        ]\n",
        "        \"\"\"\n",
        "        user_prompt = \"Messages:\\n\" + \"\\n\".join([f\"- {m}\" for m in messages])\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ]\n",
        "        )\n",
        "        try:\n",
        "            results = eval(response.choices[0].message.content)\n",
        "            if not isinstance(results, list):\n",
        "                results = []\n",
        "        except:\n",
        "            results = []\n",
        "        return results"
      ],
      "metadata": {
        "id": "w-iouUh-qC2E"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_schema = {\n",
        "    \"name\": \"string\",\n",
        "    \"email\": \"string\",\n",
        "    \"phone\": \"string\",\n",
        "    \"location\": \"string\",\n",
        "    \"age\": \"integer\"\n",
        "}\n",
        "\n",
        "sample_chats = [\n",
        "    \"Hi, my name is Alice Johnson. I live in New York, I'm 28, my email is alice@example.com, and my phone is 555-1234.\",\n",
        "    \"Hello, I'm Bob. Age 35. Email: bob123@gmail.com, Phone: 987-6543, Location: Los Angeles.\",\n",
        "    \"Hey, this is Charlie, 22 years old, living in Chicago. You can reach me at charlie22@domain.com, phone 444-555-6666.\"\n",
        "]"
      ],
      "metadata": {
        "id": "VXB0lrt3tq4-"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_info_using_groq(chats):\n",
        "    system_prompt = \"\"\"\n",
        "    You are an assistant that extracts structured information from user messages.\n",
        "    The fields to extract are: name, email, phone, location, age.\n",
        "    Respond in Python dict format only.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for chat in chats:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": chat}\n",
        "            ]\n",
        "        )\n",
        "        try:\n",
        "            data = eval(response.choices[0].message.content)\n",
        "            extracted = {k: data.get(k, \"\") for k in chat_schema.keys()}\n",
        "        except:\n",
        "            extracted = {k: \"\" for k in chat_schema.keys()}\n",
        "        results.append({\"chat\": chat, \"extracted_data\": extracted})\n",
        "    return results\n",
        "\n",
        "manager = ConversationManager(summarize_every=3)\n",
        "analyzer = ChatAnalyzerOptimized(client)\n",
        "\n",
        "conversation_messages = [\n",
        "    \"Hi, I’m looking for movie recommendations.\",\n",
        "    \"I like action movies with strong lead characters.\",\n",
        "    \"Any new releases this year?\",\n",
        "    \"I didn’t like the last movie I watched.\"\n",
        "]\n",
        "\n",
        "for msg in conversation_messages:\n",
        "    reply = manager.add_user_message(msg)\n",
        "    print(f\"User: {msg}\")\n",
        "    print(f\"Assistant: {reply}\\n\")\n",
        "\n",
        "print(\"=== Conversation History ===\")\n",
        "for msg in manager.get_history():\n",
        "    print(msg)\n",
        "\n",
        "analysis_results = analyzer.analyze_messages(conversation_messages)\n",
        "print(\"\\n=== Classification & Structured Extraction ===\")\n",
        "for r in analysis_results:\n",
        "    print(\"Message:\", r[\"message\"])\n",
        "    print(\"Category:\", r[\"category\"])\n",
        "    print(\"Extracted Data:\", r[\"extracted_data\"])\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\n=== Task 2: JSON Schema Extraction ===\")\n",
        "json_results = extract_info_using_groq(sample_chats)\n",
        "for r in json_results:\n",
        "    print(\"Chat:\", r[\"chat\"])\n",
        "    print(\"Extracted Data:\", r[\"extracted_data\"])\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg1fYLFftxFj",
        "outputId": "93d8e87d-4e07-4b0a-e5ea-0db56cbc8af4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hi, I’m looking for movie recommendations.\n",
            "Assistant: I'd be happy to help you with some movie recommendations. Do you have any specific preferences or genres you're interested in? For example, are you looking for:\n",
            "\n",
            "- Action movies\n",
            "- Romantic comedies\n",
            "- Sci-fi or fantasy films\n",
            "- Horror movies\n",
            "- Classic films\n",
            "- Recent releases\n",
            "- Animated movies\n",
            "- Something else?\n",
            "\n",
            "Or maybe there's a particular theme or theme that you're in the mood for, like historical dramas, superhero movies, or documentaries.\n",
            "\n",
            "Also, are you looking for something light-hearted and easy to watch, or something more complex and thought-provoking?\n",
            "\n",
            "User: I like action movies with strong lead characters.\n",
            "Assistant: Action movies with strong lead characters can be thrilling and engaging. Here are some recommendations that might interest you:\n",
            "\n",
            "**Recent Releases:**\n",
            "\n",
            "1. **Mad Max: Fury Road** (2015) - an adrenaline-fueled action film with a strong female lead in Charlize Theron's Imperator Furiosa.\n",
            "2. **Wonder Woman** (2017) - an iconic superhero film with a powerful and inspiring lead character played by Gal Gadot.\n",
            "3. **John Wick: Chapter 3 – Parabellum** (2019) - a non-stop action film with Keanu Reeves as the lead character.\n",
            "\n",
            "**Classic Action Films:**\n",
            "\n",
            "1. **Die Hard** (1988) - an iconic action film with Bruce Willis as a determined and resourceful lead character.\n",
            "2. **The Matrix** (1999) - a thought-provoking sci-fi action film with Keanu Reeves, Laurence Fishburne, and Carrie-Anne Moss as the leads.\n",
            "3. **Buddy Holly's biopic \"The Buddy Holly Story\" isn't an action film. I think you’re confusing this with 1998 movie. You are probably thinking  of the film,  \"Road House\" starring  Patrick Swayze (1989) which isn't a great  action film but an action romanced film.**\n",
            "\n",
            "**Other Options:**\n",
            "\n",
            "1. **Atomic Blonde** (2017) - a stylish and action-packed spy thriller with Charlize Theron as the lead character.\n",
            "2. **The Accountant** (2016) - an action-thriller film with Ben Affleck as a socially awkward, math whiz with a hidden past.\n",
            "3. **47 Ronin** (2013) - a historical action film set in feudal Japan, with Keanu Reeves as the lead character.\n",
            "\n",
            "User: Any new releases this year?\n",
            "Assistant: However, I can provide a summary of the current year's information (up to my cut-off data in 2023) and some of the recent movie releases and upcoming movie releases. Here's a brief summary:\n",
            "\n",
            "- Current Movie Releases (Up to 2023): \n",
            "\n",
            "1. \"Avatar: The Way of Water\" \n",
            "2. \"Top Gun: Maverick\"\n",
            "3. \"Everything, Everywhere, All at Once\"\n",
            "4. \"The Batman\"\n",
            "5. \"Doctor Strange in the Multiverse of Madness\"\n",
            "6. \"Thor: Love and Thunder\"\n",
            "7. \"Jurassic World Dominion\"\n",
            "8. \"Spider-Man: No Way Home\"\n",
            "9. \"Shang-Chi and the Legend of the Ten Rings\"\n",
            "\n",
            "- Upcoming Future Movies: \n",
            "\n",
            "Keep in mind that I don't have the most up-to-date information due to my cut-off data in 2023. Here are a few movies announced but not yet released, based on 2023 information: \n",
            "\n",
            "1. \"Oppenheimer\" (July 2023)\n",
            "2. \"Killers of the Flower Moon\" (October 2023)\n",
            "3. \"Dune: Part Two\" (March 2024)\n",
            "4. \"Mission: Impossible - Dead Reckoning Part Two\" (June 2023, later delayed for release)\n",
            "5. \"The Marvels\" (2023 or 2024)\n",
            "6. \"Guardians of the Galaxy Vol. 3\" (May 2023 or later)\n",
            "\n",
            "Please note that these movie release dates and information are based on my cut-off data from 2023 and might be outdated or changed since then.\n",
            "\n",
            "User: I didn’t like the last movie I watched.\n",
            "Assistant: I'm sorry to hear that you didn't enjoy the last movie you watched. Can you tell me a bit more about it? What kind of movie was it (action, comedy, drama, etc.)? Was it based on a book, comic, or original story? This might help me provide some recommendations for you in the future.\n",
            "\n",
            "=== Conversation History ===\n",
            "{'role': 'system', 'content': 'Summary: Unfortunately the information provided on new movie releases in 2023 and 2024 from this conversation is not available as the conversation has ended before that time frame could have been discussed.'}\n",
            "{'role': 'assistant', 'content': 'However, I can provide a summary of the current year\\'s information (up to my cut-off data in 2023) and some of the recent movie releases and upcoming movie releases. Here\\'s a brief summary:\\n\\n- Current Movie Releases (Up to 2023): \\n\\n1. \"Avatar: The Way of Water\" \\n2. \"Top Gun: Maverick\"\\n3. \"Everything, Everywhere, All at Once\"\\n4. \"The Batman\"\\n5. \"Doctor Strange in the Multiverse of Madness\"\\n6. \"Thor: Love and Thunder\"\\n7. \"Jurassic World Dominion\"\\n8. \"Spider-Man: No Way Home\"\\n9. \"Shang-Chi and the Legend of the Ten Rings\"\\n\\n- Upcoming Future Movies: \\n\\nKeep in mind that I don\\'t have the most up-to-date information due to my cut-off data in 2023. Here are a few movies announced but not yet released, based on 2023 information: \\n\\n1. \"Oppenheimer\" (July 2023)\\n2. \"Killers of the Flower Moon\" (October 2023)\\n3. \"Dune: Part Two\" (March 2024)\\n4. \"Mission: Impossible - Dead Reckoning Part Two\" (June 2023, later delayed for release)\\n5. \"The Marvels\" (2023 or 2024)\\n6. \"Guardians of the Galaxy Vol. 3\" (May 2023 or later)\\n\\nPlease note that these movie release dates and information are based on my cut-off data from 2023 and might be outdated or changed since then.'}\n",
            "{'role': 'user', 'content': 'I didn’t like the last movie I watched.'}\n",
            "{'role': 'assistant', 'content': \"I'm sorry to hear that you didn't enjoy the last movie you watched. Can you tell me a bit more about it? What kind of movie was it (action, comedy, drama, etc.)? Was it based on a book, comic, or original story? This might help me provide some recommendations for you in the future.\"}\n",
            "\n",
            "=== Classification & Structured Extraction ===\n",
            "Message: Hi, I’m looking for movie recommendations.\n",
            "Category: question\n",
            "Extracted Data: {'genre': '', 'character_type': ''}\n",
            "--------------------------------------------------\n",
            "Message: I like action movies with strong lead characters.\n",
            "Category: preference\n",
            "Extracted Data: {'genre': 'action', 'character_type': 'strong lead'}\n",
            "--------------------------------------------------\n",
            "Message: Any new releases this year?\n",
            "Category: question\n",
            "Extracted Data: {'genre': '', 'character_type': ''}\n",
            "--------------------------------------------------\n",
            "Message: I didn’t like the last movie I watched.\n",
            "Category: complaint\n",
            "Extracted Data: {'genre': '', 'character_type': ''}\n",
            "--------------------------------------------------\n",
            "\n",
            "=== Task 2: JSON Schema Extraction ===\n",
            "Chat: Hi, my name is Alice Johnson. I live in New York, I'm 28, my email is alice@example.com, and my phone is 555-1234.\n",
            "Extracted Data: {'name': '', 'email': '', 'phone': '', 'location': '', 'age': ''}\n",
            "--------------------------------------------------\n",
            "Chat: Hello, I'm Bob. Age 35. Email: bob123@gmail.com, Phone: 987-6543, Location: Los Angeles.\n",
            "Extracted Data: {'name': '', 'email': '', 'phone': '', 'location': '', 'age': ''}\n",
            "--------------------------------------------------\n",
            "Chat: Hey, this is Charlie, 22 years old, living in Chicago. You can reach me at charlie22@domain.com, phone 444-555-6666.\n",
            "Extracted Data: {'name': '', 'email': '', 'phone': '', 'location': '', 'age': ''}\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "import os\n",
        "os.chdir(\"/content/chatsummaryandanalyser\")\n",
        "!mv /content/groq_chat_assignment.ipynb ./groq_chat_assignment.ipynb\n",
        "!git add groq_chat_assignment.ipynb\n",
        "!git commit -m \"Update notebook with combined Task 1 and Task 2\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "nyDUwBe0uWkS",
        "outputId": "dc732b12-6ac7-463c-9aaa-2015bb236710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f63b9b42-662b-46d6-abea-b4cf1a53ffc8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f63b9b42-662b-46d6-abea-b4cf1a53ffc8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oqfpwLqgueFA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}